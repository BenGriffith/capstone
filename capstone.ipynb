{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Nanodegree Capstone Project\n",
    "\n",
    "Every year, approximately 7.6 million companion animals end up in US shelters. Many animals are given up as unwanted by their owners, while others are picked up after getting lost or taken out of cruelty situations. Many of these animals find forever families to take them home, but just as many are not so lucky. \n",
    "\n",
    "Approximately 2.7 million shelter animals are euthanized in the US every year.\n",
    "\n",
    "In this multi-class classification problem, a dataset of intake information (breed, color, sex, age, etc.) provided by the Austin Animal Center will be used to train a supervised learning algorithm. The trained model will then be utilized to help predict the outcome (adoption, died, euthanasia, return to owner or transfer) of future shelter animals.\n",
    "\n",
    "Knowing the predicted outcomes can help shelters identify and understand trends in animal outcomes. Such insights could help shelters focus their resources on specific animals who might need extra help finding a new home. For example, if the predicted outcome for a certain animal or breed in a shelter is euthanasia, the shelter could align their efforts to help see these euthanasia candidates find a new home.\n",
    "\n",
    "I intend to follow the workflow outline below as closely as possible:\n",
    "\n",
    "- Step 1: Problem Preparation\n",
    "  - Load libraries\n",
    "  - Load dataset\n",
    "\n",
    "- Step 2: Data Summarization\n",
    "  - Descriptive statistics such as .info(), .describe(), .head() and .shape\n",
    "  - Data visualization such as histograms, density plots, box plots, scatter matrix and correlation matrix\n",
    "\n",
    "- Step 3: Data Preparation\n",
    "  - Data cleaning such as handling missing values\n",
    "  - Feature preparation and data transforms such as one-hot encoding\n",
    "\n",
    "- Step 4: Evaluate Algorithm(s)\n",
    "  - Split-out validation dataset\n",
    "  - Test options and evaluation metric\n",
    "  - Spot check and compare algorithms\n",
    "\n",
    "- Step 5: Improve Algorithm(s)\n",
    "  - Algorithm tuning\n",
    "  - Compare selected algorithm against Ensembles\n",
    "\n",
    "- Step 6: Model Finalization\n",
    "  - Predictions on validation / test dataset\n",
    "  - Save model for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Preparation\n",
    "\n",
    "In this step, I am loading the necessary Python libraries and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.trees import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline\n",
    "\n",
    "# Load dataset\n",
    "filepath = 'data/train.csv'\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first five records of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the dimensions of the dataset\n",
    "print('Number of observations: %s' % data.shape[0])\n",
    "print('Number of attributes: {}'.format(data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying detailed information about dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which observations are null for the AgeuponOutcome feature\n",
    "data.AgeuponOutcome[data.AgeuponOutcome.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which observation is null for the SexuponOutcome feature\n",
    "data.SexuponOutcome[data.SexuponOutcome.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display class distribution\n",
    "data.groupby('OutcomeType').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows from dataset that have null for specified features\n",
    "data = data.dropna(subset=['AgeuponOutcome', 'SexuponOutcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into features and target variable\n",
    "y = data[['OutcomeType']]\n",
    "data = data.drop(['AnimalID', 'Name', 'DateTime', 'OutcomeType', 'OutcomeSubtype'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AgeuponOutcome to number of days\n",
    "data.AgeuponOutcome = data.AgeuponOutcome.apply(util.convertAgeToDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale AgeuponOutcome\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = pd.DataFrame(data=data)\n",
    "numerical = ['AgeuponOutcome']\n",
    "\n",
    "data_scaled[numerical] = scaler.fit_transform(data[numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement one-hot encoding for categorical features\n",
    "features_final = pd.get_dummies(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Algorithm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, y, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-check algorithms\n",
    "models = []\n",
    "models.append(('LG', LogisticRegression())) # Benchmark model \n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    print('{}: {}'.format(name, cv_results.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
